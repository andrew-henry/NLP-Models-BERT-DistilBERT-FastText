# NLP-Models-BERT-DistilBERT-FastText

BERT
--
Bidirectional Encoder Representations from Transformers is a transformer-based machine learning technique for natural language processing pre-training developed by Google. BERT was created and published in 2018 by Jacob Devlin and his colleagues from Google


DistilBERT
--



FastText
--

